{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression, also known as Tikhonov regularization, is a technique used for analyzing multiple regression data that suffer from multicollinearity. When independent variables are highly correlated, classical linear regression models' estimates can become imprecise, and the variance of the coefficient estimates can inflate significantly. Ridge regression addresses this issue by adding a degree of bias to the regression estimates, thereby reducing standard errors.\n",
    "\n",
    "### Overview of Ridge Regression\n",
    "\n",
    "The primary objective of ridge regression is to minimize the sum of the square of residuals like in ordinary least squares (OLS) regression, but with an added penalty on the size of coefficients. This penalty is the L2-norm (Euclidean norm) of the coefficients, leading to the following cost function:\n",
    "\n",
    "\\[\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\}\n",
    "\\]\n",
    "\n",
    "where \\(y_i\\) are the responses, \\(x_{ij}\\) are the predictors, \\(\\beta_j\\) are the coefficients to be estimated, \\(\\lambda\\) is the tuning parameter that controls the amount of shrinkage applied to the coefficients, \\(n\\) is the number of observations, and \\(p\\) is the number of predictors.\n",
    "\n",
    "### When to Use Ridge Regression\n",
    "\n",
    "- **Multicollinearity**: Ridge regression is particularly useful when there is a high degree of multicollinearity in the data, as it helps in stabilizing the coefficient estimates.\n",
    "- **Bias-Variance Trade-off**: When you're dealing with overfitting due to a large number of predictors, ridge regression can help in reducing model variance at the cost of a slight increase in bias, which can result in a model with better prediction accuracy on unseen data.\n",
    "- **Model Selection is not a Priority**: Unlike LASSO, which can reduce some coefficients to zero and thus perform variable selection, ridge regression will shrink coefficients close to zero but not exactly to zero. It is preferred when the goal is prediction accuracy, and all predictors are believed to be relevant or when you do not need to specifically identify which variables are most important.\n",
    "\n",
    "### Ridge vs. LASSO\n",
    "\n",
    "- **Penalty Term**: The key difference between Ridge and LASSO lies in the penalty term. Ridge regression uses the square of the magnitude of coefficients as its penalty term (\\(\\lambda \\sum \\beta_j^2\\)), whereas LASSO uses the absolute value (\\(\\lambda \\sum |\\beta_j|\\)). This difference has significant implications for the behavior of the two methods.\n",
    "- **Variable Selection**: LASSO can set some coefficients to zero, effectively performing variable selection, which is helpful when it's believed that many predictors are irrelevant or redundant. Ridge regression, however, shrinks the coefficients towards zero but never exactly to zero.\n",
    "- **Usage Scenario**: Ridge is preferable when most variables contribute to the response or when multicollinearity is present, and the number of predictors is large compared to the number of observations. LASSO is more suitable when the goal includes variable reduction/selection, or you suspect that only a subset of predictors are truly important.\n",
    "- **Tuning Parameter (\\(\\lambda\\))**: Both methods require the choice of a tuning parameter \\(\\lambda\\), which can significantly affect the performance of the model. This parameter is usually selected via cross-validation.\n",
    "\n",
    "In summary, the choice between Ridge and LASSO depends on the specific goals of your analysis (e.g., prediction accuracy vs. variable selection) and the characteristics of your data (e.g., the presence of multicollinearity or the number of predictors relative to observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "To illustrate how to use Ridge regression in R, we'll again use the `glmnet` package, which provides functionalities for fitting both Ridge and LASSO models. The example will follow a simple workflow: generating synthetic data, fitting a Ridge regression model, and selecting the best lambda value using cross-validation.\n",
    "\n",
    "### Step 1: Install and Load the `glmnet` Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate Synthetic Data\n",
    "\n",
    "Let's create a synthetic dataset for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(42) # For reproducibility\n",
    "n <- 100 # Number of observations\n",
    "p <- 10  # Number of predictors\n",
    "\n",
    "\n",
    "n <- 1000\n",
    "\n",
    "# Create a common time variable from 1 to n\n",
    "time <- seq(1, n, by = 1)\n",
    "\n",
    "# Generate sine waves of different frequencies\n",
    "X1 <- sin(2 * pi * time / 1200)  # Base sine wave\n",
    "X2 <- sin(2 * pi * time / 50)   # Higher frequency sine wave\n",
    "X3 <- sin(2 * pi * time / 400)  # Lower frequency sine wave\n",
    "\n",
    "# Introduce multicollinearity by combining sine waves\n",
    "X4 <- X1 + 0.5 * X2 + rnorm(n, mean = 0, sd = 0.1)  # Combination of X1 and X2 with noise\n",
    "X5 <- X1 + 0.5 * X3 + rnorm(n, mean = 0, sd = 0.1)  # Combination of X1 and X3 with noise\n",
    "X6 <- X2 + 0.5 * X3 + rnorm(n, mean = 0, sd = 0.1)  # Combination of X2 and X3 with noise\n",
    "\n",
    "# Additional independent variables with less correlation\n",
    "X7 <- rnorm(n, mean = 0, sd = 1)\n",
    "X8 <- runif(n, min = -1, max = 1)\n",
    "X9 <- rbinom(n, size = 1, prob = 0.5)\n",
    "X10 <- rpois(n, lambda = 3)\n",
    "\n",
    "# Combine variables into a data frame\n",
    "data <- data.frame(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)\n",
    "\n",
    "X <- data.matrix(data)\n",
    "beta <- c(1, 2, -1.5, rep(0, p-3)) # True coefficients\n",
    "y <- X %*% beta + rnorm(n) # Generate response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(42) # For reproducibility\n",
    "n <- 100 # Number of observations\n",
    "p <- 20  # Number of predictors\n",
    "\n",
    "X <- matrix(rnorm(n * p), n, p) # Predictor matrix\n",
    "beta <- c(1, 2, -1.5, rep(0, p-3)) # True coefficients\n",
    "y <- X %*% beta + rnorm(n) # Generate response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, only the first three predictors have a non-zero relationship with the response variable `y`.\n",
    "\n",
    "### Step 3: Fit a Ridge Regression Model\n",
    "\n",
    "With `glmnet`, fitting a Ridge regression model requires specifying the alpha parameter as 0 (since `alpha = 1` corresponds to LASSO and `alpha = 0` corresponds to Ridge).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convert X to a sparse matrix format expected by glmnet\n",
    "X_matrix <- as.matrix(X)\n",
    "\n",
    "# Fit Ridge regression model\n",
    "ridge_model <- glmnet(X_matrix, y, alpha = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Cross-validation to Choose Lambda\n",
    "\n",
    "The `cv.glmnet` function can be used to perform k-fold cross-validation (default is 10-fold) to find the optimal value of lambda (\\(\\lambda\\)) that minimizes the cross-validation error.\n",
    "\n",
    "This plot shows the mean squared error as a function of log(\\(\\lambda\\)), along with error bars for each value of \\(\\lambda\\). The value of \\(\\lambda\\) that minimizes the cross-validation error is considered optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(42) # For reproducibility\n",
    "cv_ridge <- cv.glmnet(X_matrix, y, alpha = 0)\n",
    "\n",
    "# Plot the cross-validation curve\n",
    "plot(cv_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract the Coefficients at Optimal Lambda\n",
    "\n",
    "You can extract the coefficients of the model fitted with the optimal lambda value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "best_lambda <- cv_ridge$lambda.min\n",
    "ridge_coeff <- coef(cv_ridge, s = best_lambda)\n",
    "print(ridge_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Make Predictions\n",
    "\n",
    "You can make predictions using the `predict` function and the model fitted with the optimal lambda:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predictions <- predict(cv_ridge, s = best_lambda, newx = as.matrix(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example covers generating synthetic data, fitting a Ridge regression model, performing cross-validation to select the optimal lambda, extracting the coefficients, and making predictions. Ridge regression is a powerful technique for handling multicollinearity and improving model generalization, especially in scenarios where the number of predictors is large relative to the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "Multicollinearity refers to a situation in statistical modeling where two or more predictor variables in a multiple regression model are highly correlated, meaning that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. In practical terms, this means that the independent variables are not truly independent from each other, which can cause several problems in the regression analysis, including:\n",
    "\n",
    "1. **Increased Variance in Coefficient Estimates**: Multicollinearity can lead to large variances and covariances for the coefficient estimates, making the estimates very sensitive to changes in the model or the data. This high variance can inflate the standard errors of the coefficients, leading to wider confidence intervals and making it harder to determine if a variable is statistically significant.\n",
    "\n",
    "2. **Unstable Coefficient Estimates**: The regression coefficients may change erratically in response to small changes in the model or the data. Small changes in the input data can lead to large changes in the model, even reversing signs of coefficients.\n",
    "\n",
    "3. **Difficulty in Interpreting Coefficients**: With multicollinearity, it becomes challenging to discern the individual impact of correlated predictors on the dependent variable because changes in one predictor are associated with changes in another.\n",
    "\n",
    "### Does Multicollinearity Affect the Bias-Variance Trade-Off?\n",
    "\n",
    "Multicollinearity primarily affects the variance part of the bias-variance trade-off, without directly influencing the bias. Here's how:\n",
    "\n",
    "- **Variance**: In the presence of multicollinearity, the variance of the coefficient estimates increases. This means that the model becomes more sensitive to small fluctuations in the training data, which can lead to overfitting. In models affected by multicollinearity, even if the overall model might seem good at predicting the training data (low bias), the high variance in the coefficient estimates can make the model perform poorly on unseen data.\n",
    "\n",
    "- **Bias**: Multicollinearity itself does not introduce bias into the coefficient estimates; the estimates are still unbiased. The problem arises because the high variance makes the estimates less reliable.\n",
    "\n",
    "### Dealing with Multicollinearity\n",
    "\n",
    "To manage multicollinearity and its impact on the variance of estimates, analysts can use several strategies:\n",
    "\n",
    "1. **Removing Highly Correlated Predictors**: By manually reviewing the correlations between variables and removing some of the highly correlated ones, you can reduce multicollinearity.\n",
    "\n",
    "2. **Principal Component Regression (PCR)**: This approach transforms the predictors into a set of linearly uncorrelated components, which can then be used in the regression analysis.\n",
    "\n",
    "3. **Ridge Regression**: This method adds a penalty equal to the square of the magnitude of coefficients to the loss function, effectively reducing the variance at the cost of introducing a small bias. This can be particularly helpful in the context of multicollinearity because it dampens the impact of highly correlated variables.\n",
    "\n",
    "In summary, while multicollinearity primarily affects the variance component of the bias-variance trade-off, strategies like ridge regression can be used to balance this trade-off by introducing a small amount of bias to significantly reduce variance, leading to models that generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for multicollinearity\n",
    "\n",
    "Is an essential step in the data preprocessing phase, especially before performing linear regression analysis. Multicollinearity can significantly affect the stability and interpretation of the regression coefficients. Several methods and indicators can help detect multicollinearity:\n",
    "\n",
    "### 1. Correlation Matrix\n",
    "A simple and initial check involves calculating the correlation coefficients between all pairs of independent variables. High correlation coefficients (either positive or negative) suggest potential multicollinearity.\n",
    "\n",
    "Look for pairs of variables with a correlation coefficient higher than 0.8 or lower than -0.8 as a basic rule of thumb, although these thresholds can be adjusted based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load necessary library\n",
    "library(MASS)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Number of observations\n",
    "n <- 100\n",
    "\n",
    "# Generate independent variables\n",
    "X1 <- rnorm(n, mean = 50, sd = 10)\n",
    "X2 <- X1 + rnorm(n, mean = 0, sd = 5)  # X2 is highly correlated with X1\n",
    "X3 <- rnorm(n, mean = 0, sd = 10) + 0.5*X1  # X3 is somewhat correlated with X1\n",
    "X4 <- rnorm(n, mean = 20, sd = 5)\n",
    "X5 <- X4 + rnorm(n, mean = 0, sd = 3)  # X5 is highly correlated with X4\n",
    "X6 <- runif(n, min = 10, max = 20)\n",
    "X7 <- runif(n, min = 5, max = 15) + 0.2*X6  # X7 is somewhat correlated with X6\n",
    "X8 <- rbinom(n, size = 1, prob = 0.5)\n",
    "X9 <- X8 + rnorm(n, mean = 0, sd = 1)  # X9 is correlated with X8\n",
    "X10 <- rpois(n, lambda = 3)\n",
    "\n",
    "# Combine variables into a data frame\n",
    "data <- data.frame(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)\n",
    "\n",
    "# Check correlations\n",
    "correlations <- cor(data)\n",
    "print(correlations)\n",
    "\n",
    "cor_matrix <- cor(X_matrix)\n",
    "print(cor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Define the number of observations\n",
    "p <- 10\n",
    "n <- 1000\n",
    "\n",
    "# Create a common time variable from 1 to n\n",
    "time <- seq(1, n, by = 1)\n",
    "\n",
    "# Generate sine waves of different frequencies\n",
    "X1 <- sin(2 * pi * time / 1100)  # Base sine wave\n",
    "X2 <- sin(2 * pi * time / 700)   # Higher frequency sine wave\n",
    "X3 <- sin(2 * pi * time / 200)  # Lower frequency sine wave\n",
    "\n",
    "# Introduce multicollinearity by combining sine waves\n",
    "X4 <- X1 + 0.5 * X2 + rnorm(n, mean = 0, sd = 0.1)  # Combination of X1 and X2 with noise\n",
    "X5 <- X1 + 0.5 * X3 + rnorm(n, mean = 0, sd = 0.1)  # Combination of X1 and X3 with noise\n",
    "X6 <- X2 + 0.5 * X3 + rnorm(n, mean = 0, sd = 0.1)  # Combination of X2 and X3 with noise\n",
    "\n",
    "# Additional independent variables with less correlation\n",
    "X7 <- rnorm(n, mean = 0, sd = 1)\n",
    "X8 <- runif(n, min = -1, max = 1)\n",
    "X9 <- rbinom(n, size = 1, prob = 0.5)\n",
    "X10 <- rpois(n, lambda = 3)\n",
    "\n",
    "# Combine variables into a data frame\n",
    "data <- data.frame(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)\n",
    "X <- data.matrix(data)\n",
    "beta <- c( 1., 0.5 , .2,  0.03,  0.03,  0.03, 0.3,  0.01,  0.01,  0.01) # True coefficients\n",
    "print(beta)\n",
    "Y <- X %*% beta # Generate response\n",
    "\n",
    "# Check correlations\n",
    "correlations <- cor(data)\n",
    "print(correlations)\n",
    "\n",
    "# Optional: Plot the sine waves to visualize their correlation\n",
    "plot(time, X1, type = 'l', col = 'red', ylim = c(-2, 2), ylab = \"Value\", xlab = \"Time\")\n",
    "lines(time, X2, col = 'blue')\n",
    "lines(time, X3, col = 'green')\n",
    "lines(time, Y, col = 'yellow')\n",
    "legend(\"topright\", legend = c(\"X1\", \"X2\", \"X3\", \"Y\"), col = c(\"red\", \"blue\", \"green\", \"yellow\"), lty = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convert X to a sparse matrix format expected by glmnet\n",
    "X_matrix <- as.matrix(X)\n",
    "\n",
    "# Fit Ridge regression model\n",
    "ridge_model <- glmnet(X_matrix, Y, alpha = 0)\n",
    "\n",
    "set.seed(42) # For reproducibility\n",
    "cv_ridge <- cv.glmnet(X_matrix, Y, alpha = 0)\n",
    "\n",
    "# Plot the cross-validation curve\n",
    "plot(cv_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lm_model <- lm(Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10, data = data)\n",
    "summary(lm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variance Inflation Factor (VIF)\n",
    "The Variance Inflation Factor (VIF) quantifies how much the variance of an estimated regression coefficient increases if your predictors are correlated. If no factors are correlated, the VIFs will all be equal to 1.\n",
    "\n",
    "A VIF value greater than 10 (or sometimes 5, depending on the source) indicates significant multicollinearity that needs to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'model' is your lm model in R\n",
    "library(car)\n",
    "vif_result <- vif(lm_model)\n",
    "print(vif_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tolerance\n",
    "Tolerance is the inverse of VIF (1/VIF) and measures the amount of variability of the selected independent variable not explained by the other independent variables. Low tolerance values (close to 0) indicate a high degree of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Condition Index\n",
    "The condition index analyzes the eigenvalues obtained from the decomposition of the predictor matrix. High condition indices (a common threshold is 30 or above) suggest multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Using the 'lm' model from above and base R functions\n",
    "model_matrix <- model.matrix(lm_model)\n",
    "eigenvalues <- eigen(crossprod(model_matrix))$values\n",
    "condition_index <- sqrt(max(eigenvalues) / eigenvalues)\n",
    "print(condition_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "```r\n",
    "\n",
    "```\n",
    "\n",
    "### 5. Eigenvalue Analysis\n",
    "Inspecting the eigenvalues of the correlation matrix or the X'X matrix (where X is the matrix of predictors) can also reveal multicollinearity. Small eigenvalues indicate the presence of multicollinearity.\n",
    "\n",
    "These methods collectively offer a comprehensive approach to diagnosing multicollinearity in your regression analysis. Addressing multicollinearity might involve removing variables, combining variables (e.g., creating a single variable from two highly correlated variables), or using regularization techniques like Ridge regression that can handle multicollinearity by penalizing large coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
